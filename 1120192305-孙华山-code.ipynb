{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'! unzip data/data136160/data.zip -d data/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''! unzip data/data136160/data.zip -d data/'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "import paddlenlp\r\n",
    "import json\r\n",
    "import paddle.nn as nn\r\n",
    "import numpy as np\r\n",
    "import jieba\r\n",
    "import time\r\n",
    "import os\r\n",
    "import paddle.nn.functional as F\r\n",
    "from functools import partial\r\n",
    "from paddlenlp.embeddings import TokenEmbedding\r\n",
    "from paddlenlp.data import JiebaTokenizer\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad, Dict\r\n",
    "from paddle.io import Dataset\r\n",
    "from tqdm import tqdm\r\n",
    "from paddlenlp.data import JiebaTokenizer\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "from paddlenlp import seq2vec #编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 参数字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramparameters_dict={'maxlength':5,'learning_rate':5e-4,'max_epoch':40,'embedding_dim':300,'output_dim':2,\\\r\n",
    "                    'eval_step':200,'batch_size':64,'log_step':70,'save_step':2000,'dropout_p':0.3,\\\r\n",
    "                    'nhead':10,'transformer_layers':6,'lstm_hidden_size':400,'encode_dim':400,'lstm_layer':2,\\\r\n",
    "                    'CNN_num_filter':128, 'ngram_filter_sizes':(3, ),\\\r\n",
    "                    'dim_feedforward':100,'save_path':'model/','data_savepath':'data_save/','encoder_name':'transformer'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "读取数据并展示部分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''#数据预处理，\r\n",
    "data_list=[]\r\n",
    "data={}\r\n",
    "new_data_list=[]\r\n",
    "posi=0\r\n",
    "nega=0\r\n",
    "path='data_new/dev.json'\r\n",
    "with open(path,'r',encoding='utf-8') as f:\r\n",
    "    for line in tqdm(f.readlines()):\r\n",
    "        d=json.loads(line)\r\n",
    "        sentence1=d['sentence1']\r\n",
    "        sentence2=d['sentence2']\r\n",
    "        lnegths=(len(sentence1)+len(sentence2))/2\r\n",
    "        new_data_list.append((lnegths,d))\r\n",
    "        data_list.append(d)\r\n",
    "f.close()\r\n",
    "#序列排序\r\n",
    "new_data_list.sort(key=lambda tup: tup[0])\r\n",
    "#写入文件：\r\n",
    "with open(path,'w+') as f:\r\n",
    "    for length,d in new_data_list:\r\n",
    "        f.write(json.dumps(d,ensure_ascii=False)+'\\n')\r\n",
    "f.close()\r\n",
    "print(\"数据集大小为：\",len(data_list))\r\n",
    "test_sample=data_list[2].copy()\r\n",
    "print(data_list[10])\r\n",
    "print('positive sample:{:},negative sample:{:}'.format(posi,nega))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义数据加载器（包含分次、停词、词嵌入等预处理）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 制作词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载paddle预训练的词嵌入模型,并不训练\r\n",
    "token_embedding = TokenEmbedding(embedding_name=\"w2v.baidu_encyclopedia.target.word-word.dim300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#制作词表的类\r\n",
    "class ToWordDict():\r\n",
    "    def __init__(self,data_path,token_embeding=token_embedding,embedding_dim=300):\r\n",
    "        self.data_path=data_path\r\n",
    "        self.token_embeding=token_embeding\r\n",
    "        self.tokenizer = JiebaTokenizer(vocab=token_embedding.vocab)\r\n",
    "        self.word_dic={}\r\n",
    "        self.word_freq_dict = dict()#记录词频\r\n",
    "        self.word2id_dict = dict()# 每个词到id的映射关系：word2id_dict\r\n",
    "        self.word2id_freq = dict()# 每个id出现的频率：word2id_freq\r\n",
    "        self.id2word_dict = dict()# 每个id到词的映射关系：id2word_dict\r\n",
    "        self.embedding_dim=embedding_dim\r\n",
    "    \r\n",
    "    \r\n",
    "    def word2id_vector(self,words,mode='id'):\r\n",
    "        if mode=='id':\r\n",
    "            return self.tokenizer.encode(words)\r\n",
    "        else:\r\n",
    "            return self.token_embeding.search(words)\r\n",
    "    \r\n",
    "    def sentence2word(self,sentence):\r\n",
    "        words=self.tokenizer.cut(sentence)\r\n",
    "        return words\r\n",
    "    \r\n",
    "    def json2sentence(self):\r\n",
    "        self.data=[]\r\n",
    "        files=os.listdir(self.data_path)\r\n",
    "        #files.remove('.ipynb_checkpoints')\r\n",
    "        for i in files:\r\n",
    "            with open(self.data_path+'/'+i,'r',encoding='utf-8') as f:\r\n",
    "                for line in f.readlines():\r\n",
    "                    d=json.loads(line)\r\n",
    "                    self.data.append(d['sentence1'])\r\n",
    "                    self.data.append(d['sentence2'])\r\n",
    "            f.close()\r\n",
    "    \r\n",
    "    def ToWordDict(self,data=None):\r\n",
    "        if data is not None:\r\n",
    "            self.data=data\r\n",
    "        for sentence in self.data:\r\n",
    "            #切词\r\n",
    "            words=self.sentence2word(sentence)\r\n",
    "            #统计并制作词典\r\n",
    "            for word in words:\r\n",
    "                if word not in self.word_freq_dict:\r\n",
    "                    self.word_freq_dict[word] = 0\r\n",
    "                self.word_freq_dict[word] += 1\r\n",
    "        #加入padding\r\n",
    "        self.word_freq_dict['PAD'] = 0\r\n",
    "        # 一般来说，出现频率高的高频词往往是：I，the，you这种代词，而出现频率低的词，往往是一些名词，如：nlp\r\n",
    "        self.word_freq_dict = sorted(self.word_freq_dict.items(), key = lambda x:x[1], reverse = True)\r\n",
    "        #构建三大词典\r\n",
    "        # 按照频率，从高到低，开始遍历每个单词，并为这个单词构造一个独一无二的id\r\n",
    "        for word, freq in self.word_freq_dict:\r\n",
    "            curr_id = len(self.word2id_dict)\r\n",
    "            self.word2id_dict[word] = curr_id\r\n",
    "            self.word2id_freq[self.word2id_dict[word]] = freq\r\n",
    "            self.id2word_dict[curr_id] = word\r\n",
    "        \r\n",
    "        # 把语料转换为id序列\r\n",
    "    def convert_corpus_to_id(self,corpus):\r\n",
    "        # 使用一个循环，将语料中的每个词替换成对应的id，以便于神经网络进行处理\r\n",
    "        corpus = [self.word2id_dict[word] for word in corpus]\r\n",
    "        return corpus\r\n",
    "    \r\n",
    "    def create_vocab_embedding(self):\r\n",
    "        self.vocab_embeddings=np.zeros((len(self.word_freq_dict), self.embedding_dim))\r\n",
    "        #print(self.word_freq_dict)\r\n",
    "        for ind, data in enumerate(self.word_freq_dict):\r\n",
    "            word,_=data\r\n",
    "            embedding=self.token_embeding.search(word)\r\n",
    "            self.vocab_embeddings[ind, :] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''twd=ToWordDict(data_path='data/data')\r\n",
    "#josn-sentence\r\n",
    "twd.json2sentence()\r\n",
    "#todict\r\n",
    "twd.ToWordDict()\r\n",
    "#制作词表\r\n",
    "twd.create_vocab_embedding()\r\n",
    "#获得词表和单词-id表\r\n",
    "vocab_embedding=twd.vocab_embeddings\r\n",
    "word2id=twd.word2id_dict\r\n",
    "print('[PAD]:',word2id['PAD'])\r\n",
    "print(vocab_embedding.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''np.save('vocab_embedding.npy',vocab_embedding)\r\n",
    "np.save('word2id.npy',word2id)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_embedding=np.load('vocab_embedding.npy')\r\n",
    "word2id=np.load('word2id.npy',allow_pickle=True)\r\n",
    "word2id=word2id.item()\r\n",
    "print(vocab_embedding.shape)\r\n",
    "print(type(word2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Word2Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#将sample中的句子分词并转换为token_id序列\r\n",
    "def convert_function(sample,token_embedding,word2id_dict):\r\n",
    "    sentence1=sample[\"sentence1\"]\r\n",
    "    sentence2=sample[\"sentence2\"]\r\n",
    "    #使用paddleAIP进行分词，词汇为预训练词表\r\n",
    "    tokenizer = JiebaTokenizer(vocab=token_embedding.vocab)\r\n",
    "    words_id_1,words_id_2=[],[]\r\n",
    "    for word in sentence1:\r\n",
    "        if word  in word2id_dict.keys():\r\n",
    "            words_id_1.append(word2id_dict[word])\r\n",
    "        else:\r\n",
    "            words_id_1.append(word2id_dict['PAD'])\r\n",
    "    for word in sentence2:\r\n",
    "        if word  in word2id_dict.keys():\r\n",
    "            words_id_2.append(word2id_dict[word])\r\n",
    "        else:\r\n",
    "            words_id_2.append(word2id_dict['PAD'])\r\n",
    "    sample[\"sentence1\"]=words_id_1\r\n",
    "    sample[\"sentence2\"]=words_id_2\r\n",
    "    if 'label' in sample.keys():\r\n",
    "        sample['label']=np.array(sample['label'],dtype=\"float32\")\r\n",
    "    return sample\r\n",
    "#分词、去停词、转换为id预处理\r\n",
    "trans_func = partial(\r\n",
    "        convert_function,\r\n",
    "        token_embedding=token_embedding,word2id_dict=word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''#trans_func示例\r\n",
    "print(trans_func(test_sample))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_embedding(words_id_list,vocab_embedding):\r\n",
    "    embeddings=[vocab_embedding[i] for i in words_id_list]\r\n",
    "    return np.array(embeddings)\r\n",
    "embedding_func=partial(word_embedding,vocab_embedding=vocab_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 序列转换为token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''original_path='data/data/train.json'\r\n",
    "target_path='data_new/train.json'\r\n",
    "with open(original_path,'r',encoding='utf-8') as f1:\r\n",
    "    with open(target_path,'w+') as f2:\r\n",
    "        for line in tqdm(f1.readlines()):\r\n",
    "            d=json.loads(line)\r\n",
    "            d=trans_func(d)\r\n",
    "            label=d['label']\r\n",
    "            d['label']=int(label)\r\n",
    "            f2.write(json.dumps(d, ensure_ascii=False)+'\\n')\r\n",
    "    f2.close()\r\n",
    "f1.close()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dataset(paddle.io.Dataset):\r\n",
    "    def __init__(self,data_path='data/data/',mode='train',trans_fun=None,word_embeding_func=None,if_offline_to_id=False):\r\n",
    "        super(Dataset, self).__init__()\r\n",
    "        self.data=[]\r\n",
    "        self.mode=mode\r\n",
    "        self.trans_func=trans_fun\r\n",
    "        self.word_embedding_func=word_embeding_func\r\n",
    "        self.if_offline_to_id=if_offline_to_id\r\n",
    "        #读取数据\r\n",
    "        data_path=data_path+self.mode+'.json'\r\n",
    "        with open(data_path,'r',encoding='utf-8') as f:\r\n",
    "            for line in f.readlines():\r\n",
    "                d=json.loads(line)\r\n",
    "                self.data.append(d)\r\n",
    "        f.close()\r\n",
    "    \r\n",
    "    def __getitem__(self,index,cut_embedding='cut'):\r\n",
    "        #更具具体需求返回分词与否、词嵌入与否的数据\r\n",
    "        if self.if_offline_to_id:\r\n",
    "            cut_embedding='no'\r\n",
    "        if cut_embedding == 'all':\r\n",
    "            sample=self.data[index]\r\n",
    "            #print(sample)\r\n",
    "            #分词去停词\r\n",
    "            sample_words=self.trans_func(sample)\r\n",
    "            #进行词嵌入\r\n",
    "            sample_embedding=dict()\r\n",
    "            sample_embedding['sentence1']=self.word_embedding_func(sample_words['sentence1'])\r\n",
    "            sample_embedding['sentence2']=self.word_embedding_func(sample_words['sentence2'])\r\n",
    "            if 'label' in sample.keys():\r\n",
    "                sample_embedding['label']=sample_words['label']\r\n",
    "            return sample_embedding\r\n",
    "        elif cut_embedding=='cut':\r\n",
    "            sample=self.data[index].copy()\r\n",
    "            #分词去停词，返回句子ID\r\n",
    "            sample_words=self.trans_func(sample)\r\n",
    "            return sample_words\r\n",
    "        else:#cut_embedding=='no'\r\n",
    "            sample=self.data[index]\r\n",
    "            return sample\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#展示部分数据\r\n",
    "'''test_dataset=Dataset(data_path='data/data/',mode='train',trans_fun=trans_func,word_embeding_func=embedding_func)\r\n",
    "print('原始数据：')\r\n",
    "print(test_dataset.__getitem__(0,cut_embedding='no'))\r\n",
    "print('分词、转换为token_id的对应数据：')\r\n",
    "print(test_dataset.__getitem__(0,cut_embedding='cut'))\r\n",
    "print('分词、去停词并进行词嵌入后的对应数据：')\r\n",
    "sampletest=test_dataset.__getitem__(0,cut_embedding='all')\r\n",
    "print(\"词嵌入后的维度：\",sampletest['sentence1'].shape)'''\r\n",
    "\r\n",
    "#线下计算token-id,直接返回原始数据即可\r\n",
    "test_dataset=Dataset(data_path='data_new/',mode='train',trans_fun=trans_func,word_embeding_func=embedding_func,if_offline_to_id=True)\r\n",
    "print('原始数据：')\r\n",
    "print(test_dataset.__getitem__(0,cut_embedding='no'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 保存数据！！！！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 定义数据集加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#构建数据加载器\r\n",
    "def create_dataloader(dataset, mode='train',batch_size=1,batchify_fn=None):\r\n",
    "    shuffle = True if mode == 'train' else False\r\n",
    "    if mode == 'train':\r\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\r\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\r\n",
    "    else:\r\n",
    "        batch_sampler = paddle.io.BatchSampler(\r\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\r\n",
    "    return paddle.io.DataLoader(dataset=dataset,batch_sampler=batch_sampler,collate_fn=batchify_fn,return_list=True)\r\n",
    "\r\n",
    "#整合Batch的数据\r\n",
    "#需要根据是否使用Bert进行调整：非Bert返回token-id，Bert则需返回token-id\\token_type等为字典。\r\n",
    "def collate_func(batch_data,if_bert=False,max_length=15):\r\n",
    "    batch_size = len(batch_data)\r\n",
    "    # 如果batch_size为0，则返回一个空字典\r\n",
    "    if batch_size == 0:\r\n",
    "        return {}\r\n",
    "    sentence1_list,sentence2_list,label_list=[],[],[]\r\n",
    "    input_ids,token_type_ids=[],[]\r\n",
    "    #判断是否是预测数据\r\n",
    "    instance1=batch_data[1]\r\n",
    "    is_test=1\r\n",
    "    if 'label' in instance1.keys():\r\n",
    "        is_test=0\r\n",
    "    for instance in batch_data:\r\n",
    "        if if_bert:\r\n",
    "            token_type_id=instance['token_type_ids']\r\n",
    "            input_id=instance['input_ids']\r\n",
    "            input_ids.append(paddle.to_tensor(input_id, dtype=\"int64\"))\r\n",
    "            token_type_ids.append(paddle.to_tensor(token_type_id, dtype=\"int64\"))\r\n",
    "        else:\r\n",
    "            sentence1=instance[\"sentence1\"]\r\n",
    "            sentence2=instance[\"sentence2\"]\r\n",
    "            #由于是token-id因此返回的是int64形\r\n",
    "            sentence1_list.append(paddle.to_tensor(sentence1, dtype=\"int64\"))\r\n",
    "            sentence2_list.append(paddle.to_tensor(sentence2, dtype=\"int64\"))\r\n",
    "        if is_test==0:\r\n",
    "            label=instance['label']\r\n",
    "        if is_test==0:\r\n",
    "            label_list.append(label)\r\n",
    "    # 对一个batch内的数据，进行padding,padding的值为词表中的[pad]的Index:635964\r\n",
    "    if if_bert:\r\n",
    "        if is_test==0:\r\n",
    "            return {'input_ids':Pad(axis=0, pad_val=tokenizer.pad_token_id,pad_right=False)(input_ids),  # input_ids\r\n",
    "                'token_type_ids':Pad(axis=0, pad_val=tokenizer.pad_token_type_id,pad_right=False)(token_type_ids),\r\n",
    "                \"labels\":Stack(dtype=\"int64\")(label_list),\r\n",
    "                }\r\n",
    "        else:\r\n",
    "            return {'input_ids':Pad(axis=0, pad_val=tokenizer.pad_token_id,pad_right=False)(input_ids),\r\n",
    "                'token_type_ids':Pad(axis=0, pad_val=tokenizer.pad_token_type_id,pad_right=False)(token_type_ids),\r\n",
    "                }\r\n",
    "    else:\r\n",
    "        if is_test==0:\r\n",
    "            return {\"sentence1s\": Pad(pad_val=10218, axis=0,pad_right=False)(sentence1_list),\r\n",
    "                \"sentence2s\": Pad(pad_val=10218, axis=0,pad_right=False)(sentence2_list),\r\n",
    "                \"labels\": Stack(dtype=\"int64\")(label_list),\r\n",
    "                }\r\n",
    "        else:\r\n",
    "            return {\"sentence1s\": Pad(pad_val=10218, axis=0,pad_right=False)(sentence1_list),\r\n",
    "                \"sentence2s\": Pad(pad_val=10218, axis=0,pad_right=False)(sentence2_list),\r\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#测试：\r\n",
    "#构建数据集\r\n",
    "batch_size=paramparameters_dict['batch_size']\r\n",
    "test_data_loader = create_dataloader(test_dataset,mode='train',batch_size=batch_size,batchify_fn=collate_func)\r\n",
    "print(test_dataset.__len__()//batch_size)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 编码器构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LSTM\r\n",
    "#Lstm使用最后一个输出编码作为整体句子的编码，对编码的句子的长度没有要求\r\n",
    "lstm_encoder=nn.LSTM(input_size=paramparameters_dict['embedding_dim'],\\\r\n",
    "                hidden_size=paramparameters_dict['encode_dim'],num_layers=2,dropout=paramparameters_dict['dropout_p'])\r\n",
    "\r\n",
    "#使用集成的LSTMEncoder\r\n",
    "lstm_encoder_api=seq2vec.LSTMEncoder(input_size=paramparameters_dict['embedding_dim'],hidden_size=paramparameters_dict['encode_dim'],\\\r\n",
    "                num_layers=paramparameters_dict['lstm_layer'],direction='bidirect')\r\n",
    "\r\n",
    "#CNN\r\n",
    "#使用集成的CNNEncoder\r\n",
    "cnn_encoder_api=seq2vec.CNNEncoder(emb_dim=paramparameters_dict['embedding_dim'],num_filter=paramparameters_dict['CNN_num_filter'],\\\r\n",
    "                ngram_filter_sizes=paramparameters_dict['ngram_filter_sizes'])\r\n",
    "\r\n",
    "#RNN集成API\r\n",
    "rnn_encoder_api=seq2vec.RNNEncoder(input_size=paramparameters_dict['embedding_dim'],\\\r\n",
    "            hidden_size=paramparameters_dict['encode_dim'],\\\r\n",
    "            num_layers=paramparameters_dict['lstm_layer'],)\r\n",
    "\r\n",
    "\r\n",
    "'''from paddlenlp.transformers import BertModel,BertTokenizer\r\n",
    "\r\n",
    "#加载预训练的Bert模型\r\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-wwm-chinese')\r\n",
    "bert_encode = BertModel.from_pretrained('bert-wwm-chinese')\r\n",
    "#Ernie\r\n",
    "tokenizer_ernie = paddlenlp.transformers.ErnieGramTokenizer.from_pretrained('ernie-gram-zh')\r\n",
    "Ernie_encode = paddlenlp.transformers.ErnieGramModel.from_pretrained('ernie-gram-zh')\r\n",
    "\r\n",
    "tokenizer=tokenizer_bert\r\n",
    "\r\n",
    "#对于Bert需要特有的转换方式等预处理方式,输入sample字典，输出是每个句子对应的input_id\\token_type_id的字典\r\n",
    "def for_Bert_transfunc(sample,tokenizer,max_seq_length=512):\r\n",
    "    query=sample['sentence1']\r\n",
    "    title=sample['sentence2']\r\n",
    "    encoded_inputs = tokenizer(\r\n",
    "        text=query, text_pair=title, max_seq_len=max_seq_length)\r\n",
    "    if 'label' in sample.keys():\r\n",
    "        encoded_inputs['label']=np.array(sample['label'],dtype=\"float32\")\r\n",
    "    return encoded_inputs\r\n",
    "\r\n",
    "bert_transfunc=partial(for_Bert_transfunc,tokenizer=tokenizer,max_seq_length=512)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#transformer编码器\r\n",
    "from paddlenlp.transformers import WordEmbedding,PositionalEmbedding\r\n",
    "class Transformer_encoder(nn.Layer):\r\n",
    "    def __init__(self,vocab_size,max_length,emb_dim,encode_dim,\\\r\n",
    "        trans_layer_num,trans_head_num,hidden_dim,normalize_before=False,if_pre_embeding=False,token_embedding=vocab_embedding ):\r\n",
    "        super(Transformer_encoder,self).__init__()\r\n",
    "        #输入：词嵌入大小，最大长度，词嵌入维度，句子编码维度，transformer层数，\\\r\n",
    "        # 多头机制头数目，FNN隐藏层大小,是否在子层输入前层归一化\r\n",
    "        #定义wordembedding和positionembeding，作为transformer的输入\r\n",
    "        \r\n",
    "        self.if_pre_embeding=if_pre_embeding\r\n",
    "        #选择是否使用预训练的词嵌入\r\n",
    "        if self.if_pre_embeding:\r\n",
    "            print('使用预训练的词向量')\r\n",
    "            pretrained_attr = paddle.ParamAttr(\r\n",
    "                                   initializer=paddle.nn.initializer.Assign(token_embedding),\r\n",
    "                                   trainable=False)\r\n",
    "            self.wordembedding=nn.Embedding(num_embeddings=token_embedding.shape[0],\r\n",
    "                                      embedding_dim=token_embedding.shape[1],\r\n",
    "                                      weight_attr=pretrained_attr)\r\n",
    "        else:\r\n",
    "            print('使用可学习的词向量')\r\n",
    "            self.wordembedding=WordEmbedding(vocab_size=vocab_size,emb_dim=emb_dim,bos_id=10218)\r\n",
    "        self.positionembedding=PositionalEmbedding(emb_dim=emb_dim,max_length=max_length)\r\n",
    "        #定义编码层\r\n",
    "        self.encoderlayer=nn.TransformerEncoderLayer(d_model=emb_dim,nhead=trans_head_num,\\\r\n",
    "            dim_feedforward=hidden_dim,normalize_before=normalize_before)\r\n",
    "        #定义编码器：\r\n",
    "        self.tansformer_encoder=nn.TransformerEncoder(self.encoderlayer,num_layers=trans_layer_num)\r\n",
    "        #定义线性层，将transformer的输出映射到编码输出\r\n",
    "        self.liner=nn.Linear(in_features=emb_dim,out_features=encode_dim)\r\n",
    "    \r\n",
    "    #将transformer encoder的每一个输出加和，作为句子表征\r\n",
    "    def concat_trans_out(self,code):\r\n",
    "        encode=paddle.zeros((code.shape[0],code.shape[2]))\r\n",
    "        for i in range(code.shape[1]):\r\n",
    "            encode=code[:,i,:]+encode\r\n",
    "        return encode\r\n",
    "\r\n",
    "    def avrage2out(self,code):\r\n",
    "        encode=paddle.zeros((code.shape[0],code.shape[2]))\r\n",
    "        for i in range(code.shape[1]):\r\n",
    "            encode=code[:,i,:]+encode\r\n",
    "        return encode/code.shape[1]\r\n",
    "\r\n",
    "    def maxpooling2out(self,code):\r\n",
    "        encode=paddle.max(code,axis=1)\r\n",
    "        return encode\r\n",
    "    \r\n",
    "    def forward(self,token_ids):\r\n",
    "        #[batch_size,sequence_length]\r\n",
    "        batch_size,sequence_length=token_ids.shape\r\n",
    "        #首先得到对饮的位置id\r\n",
    "        pos=paddle.tile(paddle.arange(start=0, end=sequence_length), repeat_times=[batch_size, 1])\r\n",
    "        #进行位置编码\r\n",
    "        pos_emb = self.positionembedding(pos)\r\n",
    "        #进行词嵌入\r\n",
    "        src_emb = self.wordembedding(token_ids)\r\n",
    "        #加和作为transformer输入\r\n",
    "        trans_input=pos_emb+src_emb\r\n",
    "        code=self.tansformer_encoder(trans_input)\r\n",
    "        #print(code)\r\n",
    "        code=self.liner(code)\r\n",
    "        #print(code.shape)\r\n",
    "        #得到句子表征\r\n",
    "        out=self.maxpooling2out(code)\r\n",
    "        return out\r\n",
    "\r\n",
    "transformer_encoder=Transformer_encoder(vocab_size=vocab_embedding.shape[0],max_length=256,emb_dim=paramparameters_dict['embedding_dim'],\\\r\n",
    "        encode_dim=paramparameters_dict['encode_dim'],trans_layer_num=paramparameters_dict['transformer_layers'],\\\r\n",
    "        trans_head_num=paramparameters_dict['nhead'],hidden_dim=paramparameters_dict['dim_feedforward'],normalize_before=True,if_pre_embeding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''test_sample={'sentence1': '支付宝系统点我的里面没有花呗这一项', 'sentence2': '我下载支付宝怎么没有花呗的', 'label': '1'}\r\n",
    "print(test_sample)\r\n",
    "max_seq_length=512\r\n",
    "encoded_inputs = bert_transfunc(test_sample)\r\n",
    "print(encoded_inputs)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''#通过Bert转换后的示例\r\n",
    "test_sample={'sentence1': '蚂蚁花呗说我违约一次', 'sentence2': '蚂蚁花呗违约行为是什么', 'label': '0'}\r\n",
    "print('原始数据：')\r\n",
    "print(test_sample)\r\n",
    "input_sample=bert_transfunc(test_sample)\r\n",
    "print('转换后数据')\r\n",
    "input_ids=paddle.to_tensor([input_sample['input_ids']],dtype='int64')\r\n",
    "token_type_ids=paddle.to_tensor([input_sample['token_type_ids']],dtype='int64')\r\n",
    "print(input_sample)\r\n",
    "print('cls向量')\r\n",
    "(sequence_output,cls)=Ernie_encode(input_ids=input_ids,token_type_ids=token_type_ids)\r\n",
    "print(cls.shape)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 分类器构建，组网"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Classifer(nn.Layer):\r\n",
    "    def __init__(self,encoder=None,embedding_dim=paramparameters_dict['embedding_dim'],\\\r\n",
    "                encoder_dim=paramparameters_dict['encode_dim'],\\\r\n",
    "                class_dim=paramparameters_dict['output_dim'],is_embedding=False,\\\r\n",
    "                token_embedding=vocab_embedding,is_pre_embedding=False,mode='LSTM',init_scale=0.1,vocab_size=vocab_embedding.shape[0],):\r\n",
    "        #参数分别的意义：编码器、词嵌入维度、编码维度、分类数目、是否进行词嵌入、词嵌入的词表、是否使用预训练词嵌入、编码器名称、词嵌入层初始化、词表大小\r\n",
    "        super(Classifer,self).__init__()\r\n",
    "        print('使用的编码器是：'+mode)\r\n",
    "        self.is_pre_embedding=is_pre_embedding\r\n",
    "        self.is_embedding=is_embedding\r\n",
    "        self.embedding_dim=embedding_dim\r\n",
    "        self.encoder_dim=encoder_dim\r\n",
    "        self.mode=mode\r\n",
    "        self.encoder=encoder\r\n",
    "        if self.is_pre_embedding and self.is_embedding:\r\n",
    "            #构建预训练的词嵌入层\r\n",
    "            print('使用预训练的词嵌入')\r\n",
    "            pretrained_attr = paddle.ParamAttr(\r\n",
    "                                   initializer=paddle.nn.initializer.Assign(token_embedding),\r\n",
    "                                   trainable=False)\r\n",
    "            self.embedding_layer=nn.Embedding(num_embeddings=token_embedding.shape[0],\r\n",
    "                                      embedding_dim=token_embedding.shape[1],\r\n",
    "                                      weight_attr=pretrained_attr)\r\n",
    "        elif self.is_embedding:\r\n",
    "            #使用非预训练的词嵌入层\r\n",
    "            print('使用非预训练的词嵌入，训练词嵌入层')\r\n",
    "            self.embedding_layer=nn.Embedding(num_embeddings=vocab_size, embedding_dim=self.embedding_dim, sparse=False, \r\n",
    "                                    weight_attr=paddle.ParamAttr(initializer=nn.initializer.Uniform(low=-init_scale, high=init_scale)))\r\n",
    "                                    \r\n",
    "        if self.mode!='Bert' and self.mode!=\"Ernie\":\r\n",
    "            self.fnn=nn.Sequential(nn.Linear(in_features=2*self.encoder_dim,out_features=400),nn.ReLU(),\\\r\n",
    "                    nn.Linear(in_features=400,out_features=class_dim))\r\n",
    "            print('使用普通的Embedding层')\r\n",
    "\r\n",
    "        else:\r\n",
    "            self.fnn=nn.Sequential(nn.Linear(in_features=self.encoder.config[\"hidden_size\"],out_features=400),nn.ReLU(),\\\r\n",
    "                    nn.Linear(in_features=400,out_features=class_dim))\r\n",
    "            print('使用Transformer-based model')\r\n",
    "        \r\n",
    "    def forward(self,input_data):\r\n",
    "        first_sentence_id,second_sentence_id=input_data[0],input_data[1]\r\n",
    "        #如果不是Bert,那么直接就是输入词向量或者token-id\r\n",
    "        #对于Bert编码器，两个sentence_id对应的是各自句子的Bert：input_id和token_type_id，需要进一步处理转换为Bert的输入，且使用Bert时，不需要embedding。\r\n",
    "        if self.is_embedding:\r\n",
    "            #进行词嵌入\r\n",
    "            first_sentence=self.embedding_layer(first_sentence_id)\r\n",
    "            second_sentence=self.embedding_layer(second_sentence_id)\r\n",
    "        else:\r\n",
    "            first_sentence=first_sentence_id\r\n",
    "            second_sentence=second_sentence_id\r\n",
    "        #计算编码\r\n",
    "        if self.mode=='Bert' or self.mode=='Ernie':\r\n",
    "            (sequence_output,cls)=self.encoder(input_ids=first_sentence,\\\r\n",
    "                                    token_type_ids=second_sentence)\r\n",
    "            #得到的CLS就可以用于文本分类\r\n",
    "            input_v=cls\r\n",
    "            out=self.fnn(input_v)\r\n",
    "            return out\r\n",
    "        \r\n",
    "        elif self.mode=='Transformer':\r\n",
    "            code1=self.encoder(first_sentence)\r\n",
    "            code2=self.encoder(second_sentence)\r\n",
    "            #合并向量：\r\n",
    "            input_v=paddle.concat(x=[code1,code2], axis=-1)\r\n",
    "            out=self.fnn(input_v)\r\n",
    "            return out\r\n",
    "        elif  self.mode=='LSTM_API':\r\n",
    "            seq_len1=paddle.to_tensor([first_sentence.shape[1] for i in range(first_sentence.shape[0])])\r\n",
    "            seq_len2=paddle.to_tensor([second_sentence.shape[1] for i in range(second_sentence.shape[0])])\r\n",
    "            code1=self.encoder(first_sentence,seq_len1)\r\n",
    "            code2=self.encoder(second_sentence,seq_len2)\r\n",
    "            #合并向量：\r\n",
    "            input_v=paddle.concat(x=[code1,code2], axis=-1)\r\n",
    "            out=self.fnn(input_v)\r\n",
    "            return out\r\n",
    "        elif self.mode=='CNN':\r\n",
    "            code1=self.encoder(first_sentence)\r\n",
    "            code2=self.encoder(second_sentence)\r\n",
    "            #合并向量：\r\n",
    "            input_v=paddle.concat(x=[code1,code2], axis=-1)\r\n",
    "            out=self.fnn(input_v)\r\n",
    "            return out\r\n",
    "        elif self.mode=='LSTM':\r\n",
    "            codes1,(h,c)=self.encoder(first_sentence)\r\n",
    "            codes2,(h,c)=self.encoder(second_sentence)\r\n",
    "            #选择句子表征\r\n",
    "            code1=self.sumtosentencevecter(codes1)\r\n",
    "            code2=self.sumtosentencevecter(codes2)\r\n",
    "             #合并向量：\r\n",
    "            input_v=paddle.concat(x=[code1,code2], axis=-1)\r\n",
    "            out=self.fnn(input_v)\r\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def alltest(model,data_loader_test):\r\n",
    "    # 测试函数，需要完成的任务有：根据测试数据集中的数据，逐个对其进行预测，生成预测值。\r\n",
    "    with open('predict_labels_1120192305.txt','w+',encoding='utf-8') as f:\r\n",
    "        model.eval()\r\n",
    "        mode=model.mode\r\n",
    "        for sample in tqdm(data_loader_test):\r\n",
    "            if mode==\"Bert\" or mode=='Ernie':\r\n",
    "                sentence1=sample['input_ids']\r\n",
    "                sentence2=sample['token_type_ids']\r\n",
    "            else:\r\n",
    "                sentence1=sample['sentence1s']\r\n",
    "                sentence2=sample['sentence2s']\r\n",
    "            #预测\r\n",
    "            input_data=[sentence1,sentence2]\r\n",
    "            pres=model(input_data)\r\n",
    "            pres=np.argmax(pres.numpy(),axis=-1)\r\n",
    "            for pre in pres:\r\n",
    "                f.write(str(pre)+'\\n')\r\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 验证函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, metric, data_loader):\r\n",
    "    model.eval()\r\n",
    "    metric.reset()\r\n",
    "    losses = []\r\n",
    "    acces=[]\r\n",
    "    f1score=[]\r\n",
    "    mode=model.mode\r\n",
    "    for sample in data_loader:\r\n",
    "        if mode==\"Bert\" or mode=='Ernie':\r\n",
    "            sentence1=sample['input_ids']\r\n",
    "            sentence2=sample['token_type_ids']\r\n",
    "        else:\r\n",
    "            sentence1=sample['sentence1s']\r\n",
    "            sentence2=sample['sentence2s']\r\n",
    "        labels=sample['labels']\r\n",
    "        input_data=[sentence1,sentence2]\r\n",
    "        logits = model(input_data)\r\n",
    "        loss = criterion(logits, labels)\r\n",
    "        losses.append(loss.numpy())\r\n",
    "        logits = F.softmax(logits, axis=1)\r\n",
    "        correct = metric.compute(logits, labels)\r\n",
    "        #f1分数\r\n",
    "        y_pred=np.argmax(logits.numpy(),axis=-1)\r\n",
    "        y_ture=labels.numpy()\r\n",
    "        f1=f1_score(y_pred=y_pred,y_true=y_ture)\r\n",
    "        f1score.append(f1)\r\n",
    "        metric.update(correct)\r\n",
    "        accu = metric.accumulate()\r\n",
    "    print(\"eval loss: %.5f, accu: %.5f,f1:%.5f\" % (np.mean(losses),accu,np.mean(f1score)))\r\n",
    "    model.train()\r\n",
    "    metric.reset()\r\n",
    "    #返回验证的损失和准确率便于数据展示\r\n",
    "    return (np.mean(losses), accu,np.mean(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_train( model, data_loader,  vali_data_loader, criterion,  optimizer,metric , scheduler=None  ):\r\n",
    "    #paddle.set_device('gpu:0')\r\n",
    "    model.train()\r\n",
    "    global_step = 0\r\n",
    "\r\n",
    "    tic_train = time.time()\r\n",
    "    num_train_epochs=paramparameters_dict['max_epoch']\r\n",
    "    log_steps=paramparameters_dict['log_step']\r\n",
    "    #log_steps=1\r\n",
    "    mode=model.mode#使用的编码器\r\n",
    "    eval_step=paramparameters_dict['eval_step']\r\n",
    "    #eval_step=20\r\n",
    "    save_step=paramparameters_dict['save_step']\r\n",
    "    train_loss,train_acc,train_f1,train_iters=[],[],[],[]\r\n",
    "    eval_loss,eval_acc,eval_iters,eval_f1=[],[],[],[]\r\n",
    "    for epoch in range(num_train_epochs):\r\n",
    "        for step,sample in enumerate(data_loader):\r\n",
    "            if mode==\"Bert\" or mode=='Ernie':\r\n",
    "                sentence1=sample['input_ids']\r\n",
    "                #print('inputid:',sentence1.shape)\r\n",
    "                sentence2=sample['token_type_ids']\r\n",
    "                #print('tokrn_type',sentence2)\r\n",
    "            else:\r\n",
    "                sentence1=sample['sentence1s']\r\n",
    "                sentence2=sample['sentence2s']\r\n",
    "            truelabels=sample['labels']\r\n",
    "            #print(truelabels.shape)\r\n",
    "            #print(type(sentence1))\r\n",
    "            input_data=[sentence1,sentence2]\r\n",
    "            outputs = model(input_data)\r\n",
    "            #print(outputs.shape)\r\n",
    "            #计算损失\r\n",
    "            loss = criterion(outputs, truelabels)\r\n",
    "            #print(loss)\r\n",
    "            outputs = F.softmax(outputs, axis=1)\r\n",
    "            correct = metric.compute(outputs, truelabels)\r\n",
    "            metric.update(correct)\r\n",
    "            acc = metric.accumulate()\r\n",
    "            #f1分数\r\n",
    "            y_pred=np.argmax(outputs.numpy(),axis=-1)\r\n",
    "            y_ture=truelabels.numpy()\r\n",
    "            f1=f1_score(y_pred=y_pred,y_true=y_ture)\r\n",
    "            #反向传播\r\n",
    "            loss.backward()\r\n",
    "            global_step += 1\r\n",
    "            # 每间隔 log_steps 输出训练指标\r\n",
    "            if global_step % log_steps == 0:\r\n",
    "                print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, accuracy: %.5f,F1-score:%.5f, speed: %.2f step/s\"\r\n",
    "                % (global_step, epoch, step, loss, acc,f1,\r\n",
    "                    log_steps / (time.time() - tic_train)))\r\n",
    "                train_iters.append(global_step)\r\n",
    "                train_acc.append(acc)\r\n",
    "                train_f1.append(f1)\r\n",
    "                train_loss.append(loss.numpy())\r\n",
    "            if global_step%eval_step==0 :\r\n",
    "                evalloss,evalacc,evalf1=evaluate(model, criterion, metric, vali_data_loader)\r\n",
    "                eval_acc.append(evalacc)\r\n",
    "                eval_loss.append(evalloss)\r\n",
    "                eval_f1.append(evalf1)\r\n",
    "                eval_iters.append(global_step)\r\n",
    "            #优化器迭代一步\r\n",
    "            optimizer.step()\r\n",
    "            optimizer.clear_grad()\r\n",
    "            if scheduler:\r\n",
    "                scheduler.step()\r\n",
    "            '''if global_step % save_step == 0:\r\n",
    "                save_path=paramparameters_dict['save_path']+paramparameters_dict['encoder_name']+str(global_step)+'.pdparams'\r\n",
    "                paddle.save(model.state_dict(),save_path)'''\r\n",
    "        metric.reset()\r\n",
    "    save_path=paramparameters_dict['save_path']+paramparameters_dict['encoder_name']+'fc_finall.pdparams'\r\n",
    "    paddle.save(model.state_dict(),save_path)\r\n",
    "    return (train_loss,train_acc,train_f1,train_iters),(eval_loss,eval_acc,eval_f1,eval_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#构建数据集\r\n",
    "batch_size=paramparameters_dict['batch_size']\r\n",
    "'''#bert模型\r\n",
    "trans_fun=bert_transfunc\r\n",
    "collate_fun=partial(collate_func,if_bert=True)\r\n",
    "train_ds=Dataset(data_path='data/data/',mode='train',trans_fun=trans_fun,word_embeding_func=embedding_func,if_offline_to_id=False)\r\n",
    "eval_ds=Dataset(data_path='data/data/',mode='dev',trans_fun=trans_fun,word_embeding_func=embedding_func,if_offline_to_id=False)'''\r\n",
    "#lstm\\transformer模型\r\n",
    "trans_fun=trans_func\r\n",
    "collate_fun=collate_func\r\n",
    "train_ds=Dataset(data_path='data_new/',mode='train',trans_fun=trans_fun,word_embeding_func=embedding_func,if_offline_to_id=True)\r\n",
    "eval_ds=Dataset(data_path='data_new/',mode='dev',trans_fun=trans_fun,word_embeding_func=embedding_func,if_offline_to_id=True)\r\n",
    "#test_ds=Dataset(data_path='data/data/',mode='test',trans_fun=trans_fun,word_embeding_func=embedding_func)\r\n",
    "print('训练集大小：',train_ds.__len__())\r\n",
    "print('验证集大小：',eval_ds.__len__())\r\n",
    "#print('测试集大小：',test_ds.__len__())\r\n",
    "train_data_loader = create_dataloader(train_ds,mode='train',batch_size=batch_size,batchify_fn=collate_fun)\r\n",
    "eval_data_loader = create_dataloader(eval_ds,mode='dev',batch_size=batch_size,batchify_fn=collate_fun)\r\n",
    "#test_data_loader = create_dataloader(test_ds,mode='test',batch_size=batch_size,batchify_fn=collate_fun)\r\n",
    "print('Batch_size:',batch_size)\r\n",
    "print('训练一轮步数：',train_ds.__len__()//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#paddle.set_device('gpu:0')\r\n",
    "#bert model\r\n",
    "#Enire model\r\n",
    "'''model=Classifer(encoder=bert_encode,is_embedding=False,mode='Bert')'''\r\n",
    "#transformer\r\n",
    "model=Classifer(encoder=transformer_encoder,is_embedding=False,mode='Transformer')\r\n",
    "#lstm\r\n",
    "'''model=Classifer(encoder=latm_encoder_api,is_embedding=True,is_pre_embedding=True,mode='transformer')'''\r\n",
    "#打印网络查看网络结构\r\n",
    "params_info = paddle.summary(model, (2, 64,5),dtypes='int64')\r\n",
    "print(params_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 定义优化器等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义损失函数：交叉熵损失\r\n",
    "criterion=nn.loss.CrossEntropyLoss()\r\n",
    "#定义优化器\r\n",
    "'''from paddlenlp.transformers import LinearDecayWithWarmup\r\n",
    "num_training_steps = len(train_data_loader) * paramparameters_dict['max_epoch']\r\n",
    "lr_scheduler = LinearDecayWithWarmup(paramparameters_dict['learning_rate'], num_training_steps, 0.0)\r\n",
    "decay_params = [\r\n",
    "    p.name for n, p in model.named_parameters()\r\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "]\r\n",
    "\r\n",
    "# 定义 Optimizer\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=lr_scheduler,\r\n",
    "    parameters=model.parameters(),\r\n",
    "    weight_decay=0.0,\r\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)'''\r\n",
    "#lstm优化器\r\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=paramparameters_dict['learning_rate'],parameters=model.parameters())\r\n",
    "print(paramparameters_dict['learning_rate'])\r\n",
    "#定义评估标准：准且率、二分了可以使用F1分数\r\n",
    "acc_metric=paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#训练验证\r\n",
    "start=time.perf_counter()\r\n",
    "paddle.set_device('gpu:0')\r\n",
    "#train_data,eval_data=do_train(model,train_data_loader,eval_data_loader,criterion,optimizer,acc_metric,lr_scheduler)\r\n",
    "train_data,eval_data=do_train(model,train_data_loader,eval_data_loader,criterion,optimizer,acc_metric)\r\n",
    "end=time.perf_counter()\r\n",
    "use_time=end-start\r\n",
    "print('训练使用了：{:.5f}'.format(use_time))\r\n",
    "#测试\r\n",
    "#alltest(model,test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''np.save(paramparameters_dict['data_savepath']+paramparameters_dict['encoder_name']+'train_loss.npy',train_data[0])\r\n",
    "np.save(paramparameters_dict['data_savepath']+paramparameters_dict['encoder_name']+'trainb _acc.npy',train_data[1])\r\n",
    "np.save(paramparameters_dict['data_savepath']+paramparameters_dict['encoder_name']+'train_f1.npy',train_data[2]) \r\n",
    "np.save(paramparameters_dict['data_savepath']+paramparameters_dict['encoder_name']+'train_iter.npy',train_data[3])\r\n",
    "\r\n",
    "np.save(paramparameters_dict['data_savepath']+paramparameters_dict['encoder_name']+'eval_loss.npy',eval_data[0])\r\n",
    "np.save(paramparameters_dict['data_savepath']+paramparameters_dict['encoder_name']+'eval_acc.npy',eval_data[1])\r\n",
    "np.save(paramparameters_dict['data_savepath']+paramparameters_dict['encoder_name']+'eval_f1.npy',eval_data[2])\r\n",
    "np.save(paramparameters_dict['data_savepath']+paramparameters_dict['encoder_name']+'eval_iter.npy',eval_data[3])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('训练集最佳准确率：{:.4f}'.format(np.max(train_data[1])))\r\n",
    "print('验证集最佳准确率：{:.4f}'.format(np.max(eval_data[1])))\r\n",
    "print('训练集最佳F1分数：{:.4f}'.format(np.max(train_data[2])))\r\n",
    "print('验证集最佳F1分数：{:.4f}'.format(np.max(eval_data[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \r\n",
    "plt.plot(train_data[3],train_data[0],label='train_loss')\r\n",
    "plt.plot(train_data[3],train_data[1],label='train_acc')\r\n",
    "plt.plot(train_data[3],train_data[2],label='train_f1')\r\n",
    "plt.legend()\r\n",
    "plt.title('encode is '+paramparameters_dict['encoder_name']+\":train_process\")\r\n",
    "plt.ylabel('accuracy/loss/f1_score')\r\n",
    "plt.xlabel('step')\r\n",
    "plt.ylim(0,1)\r\n",
    "plt.grid()\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(eval_data[3],eval_data[0],label='eval_loss')\r\n",
    "plt.plot(eval_data[3],eval_data[1],label='eval_acc')\r\n",
    "plt.plot(eval_data[3],eval_data[2],label='eval_f1score')\r\n",
    "plt.legend()\r\n",
    "plt.title('encode is '+paramparameters_dict['encoder_name']+\":eval_process\")\r\n",
    "plt.ylabel('accuracy/loss/f1_score')\r\n",
    "plt.xlabel('step')\r\n",
    "plt.ylim(0,1)\r\n",
    "plt.grid()\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(train_data[3],train_data[0],label='train_loss')\r\n",
    "plt.plot(eval_data[3],eval_data[0],label='eval_loss')\r\n",
    "plt.plot(train_data[3],train_data[1],label='train_acc')\r\n",
    "plt.plot(eval_data[3],eval_data[1],label='eval_acc')\r\n",
    "plt.plot(train_data[3],train_data[2],label='train_f1')\r\n",
    "plt.plot(eval_data[3],eval_data[2],label='eval_f1')\r\n",
    "plt.legend()\r\n",
    "plt.title('encode is '+paramparameters_dict['encoder_name']+\":all_process\")\r\n",
    "plt.ylabel('accuracy/loss/f1')\r\n",
    "plt.xlabel('step')\r\n",
    "plt.ylim(0,1)\r\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 样本预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.eval()\r\n",
    "label_dict={0:'不相似',1:'相似'}\r\n",
    "test_example={\"sentence1\": \"本月花呗为什么不能分期\", \"sentence2\": \"花呗分期为什么不能红包付款\"}\r\n",
    "ture_label='不相似'\r\n",
    "inputs=trans_fun(test_example)\r\n",
    "#print(inputs)\r\n",
    "inputs_=[paddle.to_tensor([inputs['sentence1']]),paddle.to_tensor([inputs['sentence2']])]\r\n",
    "pre=model(inputs_)\r\n",
    "pre=F.softmax(pre,axis=1)\r\n",
    "pre_label=np.argmax(pre.numpy()[0])\r\n",
    "print('句子1为：'+\"本月花呗为什么不能分期\")\r\n",
    "print('句子2为：'+\"花呗分期为什么不能红包付款\")\r\n",
    "print('两个句子实际'+ture_label)\r\n",
    "print('-----------------------------------------------')\r\n",
    "print('编码器为:'+paramparameters_dict['encoder_name'])\r\n",
    "print('两个类别概率为:',pre.numpy()[0])\r\n",
    "print('预测为：'+label_dict[pre_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
